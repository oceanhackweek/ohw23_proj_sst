{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054ad30-0d97-45f7-8aa5-108068718458",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Developed by Juirai, Boris, Hao, combined by Paula Birocchi \n",
    "\n",
    "date: 09-08-2023\n",
    "\n",
    "Ocean Hack Week 2023\n",
    "\n",
    "This script was developed to run a machine learning model to predict SST surface distribution.\n",
    "\n",
    "'''\n",
    "# getting the libraries necessary to run this script:\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import calendar\n",
    "import os.path\n",
    "import dask.array as da\n",
    "from dask.delayed import delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from tensorflow.keras import layers, regularizers, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Add, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Core libraries for this tutorial\n",
    "from eosdis_store import EosdisStore # Available via `pip install zarr zarr-eosdis-store`\n",
    "import requests\n",
    "from pqdm.threads import pqdm\n",
    "from matplotlib import animation, pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "from pprint import pprint\n",
    "# importing library for s3 buckets\n",
    "import s3fs\n",
    "\n",
    "# Bypass AWS tokens, keys etc.\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Verify that we're in the right place\n",
    "# getting the sattelite data from S3 buckets: \n",
    "sst_files = s3.ls(\"mur-sst/zarr-v1/\")\n",
    "sst_files\n",
    "\n",
    "ds = xr.open_zarr(\n",
    "        store=s3fs.S3Map(\n",
    "            root=f\"s3://{sst_files[0]}\", s3=s3, check=False\n",
    "        )\n",
    ")\n",
    "# We reduced the size of matrix to test the model, as we were having issues:\n",
    "# slicing our data to be able to make the model run:\n",
    "dscut = ds.sel(time=slice(\"2002-06-01\", \"2002-06-30\"),lat=slice(5,7),lon=slice(50,52))\n",
    "\n",
    "\n",
    "\n",
    "dscut['time'] = dscut['time'].dt.floor('D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e32128-063b-44af-b6b6-8bd3749034f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.delayed import delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "def preprocess_day_data(day_data):\n",
    "    day_data = da.squeeze(day_data)\n",
    "    mean_val = da.nanmean(day_data).compute()  # compute here to get scalar value\n",
    "    return day_data - mean_val\n",
    "\n",
    "def preprocess_data(zarr_ds, chunk_size=200):\n",
    "    total_len = zarr_ds['analysed_sst'].shape[0]\n",
    "    chunk_shape = (chunk_size,) + zarr_ds['analysed_sst'].shape[1:]  # Adjusted chunking\n",
    "    chunks = []\n",
    "\n",
    "    for start_idx in range(0, total_len, chunk_size):\n",
    "        end_idx = min(start_idx + chunk_size, total_len)\n",
    "        \n",
    "        # Directly slice the dask array without wrapping it with da.from_array again\n",
    "        chunk = zarr_ds['analysed_sst'][start_idx:end_idx]\n",
    "        \n",
    "        processed_chunk = chunk.map_blocks(preprocess_day_data)\n",
    "        \n",
    "        # Use da.where to replace NaNs with 0.0\n",
    "        processed_chunk = da.where(da.isnan(processed_chunk), 0.0, processed_chunk)\n",
    "        \n",
    "        chunks.append(processed_chunk)\n",
    "\n",
    "    return da.concatenate(chunks, axis=0)\n",
    "\n",
    "# processed_data = preprocess_data(zarr_ds).compute()\n",
    "processed_data = preprocess_data(dscut)\n",
    "\n",
    "def prepare_data_from_processed(processed_data, window_size=5): \n",
    "    length = processed_data.shape[0]\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(length - window_size):\n",
    "        X.append(processed_data[i:i+window_size])\n",
    "        y.append(processed_data[i+window_size])\n",
    "\n",
    "    X, y = da.array(X), da.array(y)\n",
    "    return X, y\n",
    "\n",
    "X, y = prepare_data_from_processed(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13a70a-a01d-4b3d-b9b4-eedb7772c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_split(X, y, train_ratio=0.7, val_ratio=0.2):\n",
    "    total_length = X.shape[0]\n",
    "    \n",
    "    # Compute end indices for each split\n",
    "    train_end = int(total_length * train_ratio)\n",
    "    val_end = int(total_length * (train_ratio + val_ratio))\n",
    "    \n",
    "    X_train = X[:train_end]\n",
    "    y_train = y[:train_end]\n",
    "    \n",
    "    X_val = X[train_end:val_end]\n",
    "    y_val = y[train_end:val_end]\n",
    "    \n",
    "    X_test = X[val_end:]\n",
    "    y_test = y[val_end:]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = time_series_split(X, y)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Conv2D\n",
    "\n",
    "# please change the number of points in your matrix before running the following code:\n",
    "def create_simple_model(input_shape=(5, 201,201, 1)):\n",
    "    model = Sequential()\n",
    "\n",
    "    # ConvLSTM layer\n",
    "    model.add(ConvLSTM2D(filters=32, kernel_size=(3, 3),\n",
    "                         input_shape=input_shape,\n",
    "                         padding='same', return_sequences=False))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Conv2D layer for output\n",
    "    model.add(Conv2D(filters=1, kernel_size=(3, 3), padding='same', activation='linear'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_simple_model()\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1ec1f-b31b-44b9-a4c0-ac43c1e7cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=20, validation_data=val_dataset, callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed173303-136c-4305-82eb-83c9af8a3807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_land_mask(data): \n",
    "    land_mask = np.isnan(data)\n",
    "    return np.flipud(land_mask)\n",
    "\n",
    "land_mask_resized = create_land_mask(X[0][0].compute())\n",
    "\n",
    "np.save('land_mask_resized.npy', land_mask_resized)\n",
    "def preprocess_vis_input_data(day_data):\n",
    "    day_data = np.squeeze(day_data)\n",
    "    mean_val = np.nanmean(day_data)\n",
    "    processed_data = day_data - mean_val\n",
    "    # Replace NaNs with 0.0\n",
    "    processed_data = np.where(np.isnan(processed_data), 0.0, processed_data)\n",
    "    return processed_data\n",
    "\n",
    "def postprocess_prediction(prediction, input_data,land_mask_resized):\n",
    "    # Find positions where the last day of input_data is 0\n",
    "    \n",
    "    # Set those positions in the prediction to NaN\n",
    "    prediction[land_mask_resized] = np.nan\n",
    "    \n",
    "    # Add back the historical mean\n",
    "    mean_val = np.nanmean(input_data)\n",
    "    prediction = np.where(np.isnan(prediction), np.nan, prediction + mean_val)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def predict_and_plot(date_to_predict, window_size, model, dataset, plot=True):\n",
    "    # Step 1: Select the time window\n",
    "    time_index = np.where(dataset['time'].values == np.datetime64(date_to_predict))[0][0]\n",
    "    input_data_raw = dataset['analysed_sst'][time_index-window_size:time_index].values\n",
    "    true_output_raw = dataset['analysed_sst'][time_index].values\n",
    "    print(input_data_raw.shape)\n",
    "    print(true_output_raw.shape)\n",
    "    # Preprocess the input data\n",
    "    input_data = np.array([preprocess_vis_input_data(day) for day in input_data_raw])\n",
    "    \n",
    "    # Step 2: Make prediction\n",
    "    prediction = model.predict(input_data[np.newaxis, ...])[0]\n",
    "    \n",
    "    # Postprocess the prediction\n",
    "    prediction_postprocessed = postprocess_prediction(prediction, input_data_raw,land_mask_resized)\n",
    "    print(prediction_postprocessed.shape)\n",
    "    # Step 3: Visualize\n",
    "    if plot:\n",
    "        # Determine common scale for all plots\n",
    "        input_data_raw = input_data_raw[..., np.newaxis]\n",
    "        true_output_raw = true_output_raw[np.newaxis, ..., np.newaxis]\n",
    "        prediction_postprocessed = prediction_postprocessed[np.newaxis, ...]\n",
    "        \n",
    "        all_data = np.concatenate([input_data_raw, prediction_postprocessed, true_output_raw])\n",
    "        vmin = np.nanmin(all_data)\n",
    "        vmax = np.nanmax(all_data)\n",
    "        \n",
    "        def plot_sample(sample,i, title=''):\n",
    "            sample_2d = np.squeeze(sample)\n",
    "            plt.imshow(sample_2d, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "            plt.title(title)\n",
    "            plt.colorbar()\n",
    "            plt.savefig('figure1_test'+str(i)+'.png', bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "        # show input frames\n",
    "        for i, frame in enumerate(input_data_raw):\n",
    "            plot_sample(frame, i,title=f'Input Frame {i+1} ({dataset[\"time\"].values[time_index-window_size+i]})')\n",
    "        \n",
    "        # show predicted output\n",
    "        plot_sample(prediction_postprocessed,i+1, title=f'Predicted Output ({date_to_predict})')\n",
    "        \n",
    "        # show true output\n",
    "        plot_sample(true_output_raw, i+2,title=f'True Output ({date_to_predict})')\n",
    "\n",
    "    return input_data_raw, prediction_postprocessed, true_output_raw\n",
    "                \n",
    "\n",
    "\n",
    "def compute_mae(y_true, y_pred):\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]))\n",
    "\n",
    "date_to_predict = '2002-06-30'\n",
    "window_size = 5\n",
    "input_data, predicted_output, true_output = predict_and_plot(date_to_predict, window_size, model, dscut)\n",
    "\n",
    "predicted_mae = compute_mae(true_output, predicted_output)\n",
    "print(f\"MAE between Predicted Output and True Output: {predicted_mae}\")\n",
    "\n",
    "last_input_frame = input_data[-1]\n",
    "last_input_frame_2d = np.squeeze(last_input_frame)\n",
    "true_output_2d = np.squeeze(true_output)\n",
    "last_frame_mae = compute_mae(true_output_2d, last_input_frame_2d)\n",
    "print(f\"MAE between Last Input Frame and True Output: {last_frame_mae}\")\n",
    "\n",
    "#model.save('ConvLSTM_nc_2002-.keras')\n",
    "                \n",
    "# just plotting land mask\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = np.load('land_mask_resized.npy')\n",
    "plt.imshow(data, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title('Land Mask')\n",
    "plt.savefig('land_mask_version2.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
